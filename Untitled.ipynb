{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3c021c",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1000 is out of bounds for axis 1 with size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 110>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    141\u001b[0m Sk_new \u001b[38;5;241m=\u001b[39m Sk_old\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m alpha\u001b[38;5;241m*\u001b[39m(ph_ps\u001b[38;5;129m@ph_pd\u001b[39m\u001b[38;5;241m*\u001b[39mdfdd)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Making non-linear correction to Sk, returns updated X\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m x[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m, k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m NewtonRalphson(np\u001b[38;5;241m.\u001b[39mvstack((Sk_new, dk_new)))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, )\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Update GRG and iteration counter\u001b[39;00m\n\u001b[0;32m    147\u001b[0m dfdd \u001b[38;5;241m=\u001b[39m redGrad(x[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m, k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1000 is out of bounds for axis 1 with size 1000"
     ]
    }
   ],
   "source": [
    "# MAE 598 - HW4\n",
    "# Benjamin Webb\n",
    "# 11/5/2022\n",
    "\n",
    "import numpy as np\n",
    "#import torch\n",
    "\n",
    "def minFun(x):\n",
    "\t# Function to be minimized\n",
    "\t# x: 3x1 vector of input values\n",
    "\t# Returns f(x)\n",
    "\n",
    "\treturn x[0]**2 + x[1]**2 + x[2]**2\n",
    "\n",
    "def contraints(x):\n",
    "\t# Tensor of equality constraints\n",
    "\t# x: 3x1 vector of decision and state variabls\n",
    "\t# Returns h(x)\n",
    "\n",
    "\th = np.zeros((2, 1), dtype=np.single)\n",
    "\th[0] = (1.0/4.0)*x[0]**2 + (1.0/5.0)*x[1]**2 + (1.0/25.0)*x[2]**2 - 1.0\n",
    "\th[1] = x[0] + x[1] - x[2]\n",
    "\n",
    "\treturn h\n",
    "\n",
    "def NewtonRalphson(x):\n",
    "\t# Newton-Ralphson method for non-linear system of eqs\n",
    "\n",
    "\th = contraints(x)\n",
    "\tdhds = np.zeros((2, 2), dtype=np.single)\n",
    "\tS_new = np.zeros((2, 1), dtype=np.single)\n",
    "\tS_old = np.zeros((2, 1), dtype=np.single)\n",
    "\tj = np.uint8(1)\n",
    "\tS_new[0] = x[0]\n",
    "\tS_new[1] = x[1]\n",
    "\tS_old[0] = x[0]\n",
    "\tS_old[1] = x[1]\n",
    "\tnew_x = x\n",
    "\n",
    "\twhile np.linalg.norm(h) > 0.001 and j < 100:\n",
    "\n",
    "\t\t# Calculate gradient w.r.t. s at current step\n",
    "\t\tdhds[0, 0] = 0.5 * S_new[0]\n",
    "\t\tdhds[0, 1] = 0.4 * S_new[1]\n",
    "\t\tdhds[1, 0] = 1.0\n",
    "\t\tdhds[1, 1] = 1.0\n",
    "\n",
    "\t\t# Update solution\n",
    "\t\tS_new = S_old - np.linalg.inv(dhds) @ h\n",
    "\n",
    "\t\t# Update constraints\n",
    "\t\tnew_x = np.vstack((S_new, x[2]))\n",
    "\t\th = contraints(new_x)\n",
    "\n",
    "\t\t# Update old solution\n",
    "\t\tS_old = S_new\n",
    "\n",
    "\t\t# Update iteration Counter\n",
    "\t\tj += 1\n",
    "\n",
    "\treturn new_x\n",
    "\n",
    "def redGrad(x):\n",
    "\t# Calculate reduced gradient\n",
    "\t# x: 3x1 vector\n",
    "\n",
    "\tpf_pd = np.asarray(2.0*x[2], dtype=np.single).reshape(1, 1)\n",
    "\tpf_ps = np.asarray([2.0*x[0], 2.0*x[1]], dtype=np.single).reshape(1, 2)\n",
    "\tph_ps = np.linalg.inv(np.asarray([[0.5*x[0], 0.4*x[1]], [1.0, 1.0]], dtype=np.single)).reshape(2, 2)\n",
    "\tph_pd = np.asarray([[0.08*x[2]], [-1.0]], dtype=np.single).reshape(2, 1)\n",
    "\n",
    "\ttest = pf_pd - pf_ps @ ph_ps @ ph_pd\n",
    "\treturn pf_pd - pf_ps @ ph_ps @ ph_pd\n",
    "\n",
    "def linesearch(x, dfdd):\n",
    "\t# perform linesearch\n",
    "\n",
    "\t# Define parameters\n",
    "\talpha = 1.0\n",
    "\tb = 0.5\n",
    "\tt = 0.3\n",
    "\tf = np.zeros((100, 1), dtype=np.single)\n",
    "\tphi = np.zeros((100, 1), dtype=np.single)\n",
    "\tj = np.uint8(0)\n",
    "\tf[0] = minFun(x)\n",
    "\tph_ps = np.linalg.inv(np.asarray([[0.5 * x[0], 0.4 * x[1]], [1.0, 1.0]]))\n",
    "\tph_pd = np.asarray([[0.08 * x[2]], [-1.0]])\n",
    "\n",
    "\twhile f[j] > phi[j] and j < 100:\n",
    "\n",
    "\t\t# Update iteration counter\n",
    "\t\tj += 1\n",
    "\n",
    "\t\t# Determine inputs for f(alpha)\n",
    "\t\tdk_step = x[2] - alpha*dfdd\n",
    "\t\tsk_step = x[0:2].reshape(2, 1) + alpha*(ph_ps@ph_pd*dfdd)\n",
    "\n",
    "\t\tf[j] = minFun(np.vstack((sk_step, dk_step)))\n",
    "\n",
    "\t\t# Determine phi(alpha)\n",
    "\t\tphi[j] = minFun(x) - alpha*t*dfdd\n",
    "\n",
    "\t\t# Update alpha\n",
    "\t\tif f[j] > phi[j]:\n",
    "\t\t\talpha = b*alpha\n",
    "\n",
    "\treturn alpha\n",
    "\n",
    "# Main progam code\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\t# Find initial point\n",
    "\tx = np.zeros((3, 1000), dtype=np.single)\n",
    "\tx[2, 0] = 3.0                               # Initial guess of decision variable\n",
    "\tx[0, 0] = 1.0\n",
    "\tx[1, 0] = 2.0\n",
    "\n",
    "\t# Initialize/starting point\n",
    "\tx[:, 0] = NewtonRalphson(x[:, 0]).reshape(-1, )\n",
    "\n",
    "\t# Calculate initial reduced gradient\n",
    "\tdfdd = redGrad(x[:, 0])\n",
    "\n",
    "\t# Begin GRG loop\n",
    "\tdk_new = np.single(0)\n",
    "\tdk_old = x[2, 0]\n",
    "\tSk_new = np.single(0)\n",
    "\tSk_old = x[0:2, 0]\n",
    "\tk = np.uint16(0)\n",
    "\twhile np.linalg.norm(dfdd, ord=2) > 0.001:\n",
    "\n",
    "\t\t# Determine alpha\n",
    "\t\talpha = linesearch(x[:, k], dfdd)\n",
    "\n",
    "\t\t# Take step in decision space\n",
    "\t\tdk_new = dk_old - alpha*dfdd\n",
    "\n",
    "\t\t# Take linear step in state space\n",
    "\t\tph_ps = np.linalg.inv(np.asarray([[0.5*x[0, k], 0.4*x[1, k]], [1.0, 1.0]], dtype=np.single))\n",
    "\t\tph_pd = np.array([[0.08*x[2, k]], [-1.0]], dtype=np.single)\n",
    "\t\tSk_new = Sk_old.reshape(2, 1) + alpha*(ph_ps@ph_pd*dfdd)\n",
    "\n",
    "\t\t# Making non-linear correction to Sk, returns updated X\n",
    "\t\tx[0:3, k+1] = NewtonRalphson(np.vstack((Sk_new, dk_new))).reshape(-1, )\n",
    "\n",
    "\t\t# Update GRG and iteration counter\n",
    "\t\tdfdd = redGrad(x[0:3, k+1])\n",
    "\t\tk += 1\n",
    "\n",
    "\tprint('x1: %.3f' % (x[0, k]))\n",
    "\tprint('x2: %.3f' % (x[1, k]))\n",
    "\tprint('x3: %.3f' % (x[2, k]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
